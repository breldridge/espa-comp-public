# Converts the available wind/solar profile to a scaled version by BA (balancing authority)import pandas as pdimport numpy as npimport jsonfrom os.path import joinimport timeimport datetimefrom scipy.interpolate import CubicSplineimport matplotlib.pyplot as pltimport read_timeseries_configs as rtcnp.random.seed(1129)def hour_to_five(hourly, force_zero=False):    ''' Converts hourly data to five minute data with spline interpolation '''    x = np.arange(len(hourly))    spline = CubicSpline(x, hourly)    xdense = np.arange(0,len(hourly),1./12)    fit = spline(xdense)    # Trim extreme values    fit[fit<0] = 0    fit[fit>np.max(hourly)] = np.max(hourly)    if force_zero:        # If both adjacent hourly values are zero, force the spline fit to zero (simulating night)        zero_check = np.zeros(len(fit))        for i in range(len(hourly)):            if i == 0:                imin = 0                imax = i+1            elif i == len(hourly):                imax = i                imin = i-1            else:                imin = i-1                imax = i+1            zero_check[12*imin:12*imax] = np.mean(hourly[imin:imax])        fit[zero_check == 0] = 0    return fitdef five_to_hour(five):    ''' Converts five minute data to hourly with averaging '''    hourly = np.zeros(int(len(five)/12))    for i in range(len(hourly)):        imn, imx = 12*i, 12*(i+1)        if imx > len(five):            imx = len(five)        hourly[i] = np.mean(five[imn:imx])    return hourlydef median_filter(hourly):    ''' Runs a median filter on hourly data to remove occasional erroneous spikes in output '''    if np.max(hourly) == 0:        return hourly    # Use the histogram to help find outliers (there are some invalid data points)    nbins = 100    if len(np.unique(hourly))<nbins:        nbins = int(len(np.unique(hourly))/2)        print(f"Changing to {nbins} bins")    count, value = np.histogram(hourly, bins=nbins)    hmask = np.ones(len(hourly), dtype=bool)    # Check histogram for zero values    cut_arr = np.where(count==0)[0]    if len(cut_arr) > 1:        # Require 2 consecutive zero bins        cut_diff = np.ediff1d(cut_arr)        if len(cut_diff) > 0:            if np.min(cut_diff) == 1:                cut_idx = np.where(cut_diff==1)[0][0]                cut_bin = cut_arr[cut_idx]                cut_val = value[cut_bin]                hmask = hourly < cut_val    # Now replace outliers with average of adjacent points, when possible (else zero)    for i in range(1,len(hourly)-1):        if hmask[i] == 0:            if hmask[i-1] != 0:                hlow = hourly[i-1]            else:                hlow = None            if hmask[i+1] != 0:                hhi = hourly[i+1]            else:                hhi = None            if hlow != None and hhi != None:                hourly[i] = (hlow+hhi)/2            else:                hourly[i] = 0    # plt.hist(hourly,bins=100)    # plt.show()    return hourlydef get_eia(eia_df, ba, nhrs, n5min, first_time, utc_to_pst, force_zero=False):    ''' Returns hourly and five minute data from the eia data '''    print(f"looking for ba {ba} in EIA data.")    if ba in eia_df['eia_ba_name'].values:        ba_df = eia_df[eia_df['eia_ba_name'] == ba]        ba_df = ba_df.sort_values('time_stamp')        ba_df.index = np.arange(ba_df.shape[0])        if (ba == 'PSEI' or ba == 'WALC') and force_zero:            # PSEI and WALC first full year of solar is 2022            first_time = datetime.datetime(2022, 1, 1, 0, 0)            first_time = first_time.replace(tzinfo=datetime.timezone.utc)        first_idx = ba_df[ba_df['time_stamp']==first_time].index[0] - utc_to_pst        ba_hourly = ba_df.loc[first_idx:first_idx+nhrs-1,'value']        # Turn nan to zero and filter for high excess points        ba_hourly[np.isnan(ba_hourly)] = 0        ba_hourly = median_filter(ba_hourly.values)        ba_hourly[ba_hourly < 0] = 0 # Enforce non-negativity        ba_5min = hour_to_five(ba_hourly, force_zero=force_zero)        # Add in red noise so 5-min isn't perfectly smooth        if force_zero:            rtype = 'solar'        else:            rtype = 'wind'        ba_5min = create_forecast(ba_5min, rtype, 5)    else:        ba_hourly = np.zeros(nhrs)        ba_5min = np.zeros(n5min)    return ba_hourly, ba_5mindef autocor_norm(av,st,le,au):    # average, standard deviation, length, autocorrelation    rn = np.random.normal(av,st,le)    b = np.sqrt(1-au**2)    rn_au = np.zeros(le)    rn_au[0] = rn[0]    for i in range(1,le):        rn_au[i] = au*rn_au[i-1]+b*rn[i]    return rn_audef create_forecast(data, rtype, dt):    ''' Adds sythetic noise to the data to make a "forecast".        Noise characteristics vary by resource type and time horizon        dt is either 5 or 60    '''    # Characteristics are approximate, drawn from Allison Campbell's info from WECC 2030 ADS    # and/or empirical tools from BPA and CAISO data.    mu = 0    std_dict = {'solar': 0.065, 'wind': 0.08}    au_dict = {'solar60': 0.8, 'solar5': 0.95, 'wind60': 0.9, 'wind5': 0.98}    red_noise = autocor_norm(mu,std_dict[rtype],len(data),au_dict[f'{rtype}{dt}'])    # +/- 4.5 sigma truncation    red_noise[red_noise>4.5*std_dict[rtype]] = 4.5*std_dict[rtype]    red_noise[red_noise<-4.5*std_dict[rtype]] = -4.5*std_dict[rtype]    # Add red noise and truncate low/high values    cap = np.max(data)    forecast = data + cap*red_noise    forecast[forecast < 0] = 0    forecast[forecast > cap] = cap    if rtype == 'solar':        forecast[data == 0] = 0    return forecast# Load BA nameswith open("../ba_names.json", "r") as f:    ba_names = json.load(f)    ba_names = sorted(ba_names['ba'])# Link to which profile to use from the EIA, BPA, and CAISO filesuse_ba_sol = {}use_ba_wind = {}for ba in ba_names:    if ba[:2] == 'CI':        use_ba_sol[ba] = 'CAISO'        use_ba_wind[ba] = 'CAISO'    elif ba[:2] == 'IP':        use_ba_sol[ba] = 'IPCO'        use_ba_wind[ba] = 'IPCO'    elif ba[:2] == 'PA' and ba != 'PACW':        use_ba_sol[ba] = 'PACE'        use_ba_wind[ba] = 'PACE'    elif ba == 'SPPC':        use_ba_sol[ba] = 'NEVP'        use_ba_wind[ba] = 'NEVP'    else:        use_ba_sol[ba] = ba        use_ba_wind[ba] = ba# Do a second loop, manually add in areas with missing solar or wind profilesfor ba in ba_names:    if ba == 'AZPS':        use_ba_wind[ba] = 'PNM'    elif ba == 'EPE':        use_ba_wind[ba] = 'TEPC'    elif ba == 'PGE':        use_ba_sol[ba] = 'IPCO'        use_ba_wind[ba] = 'IPCO'    elif ba == 'PSEI':        use_ba_sol[ba] = 'NWMT'    elif ba == 'TH_Mead':        use_ba_sol[ba] = 'PNM'        use_ba_wind[ba] = 'LDWP'    elif ba == 'TH_PV':        use_ba_sol[ba] = 'SRP'    elif ba == 'TIDC':        use_ba_sol[ba] = 'NEVP'    elif ba == 'VEA':        use_ba_sol[ba] = 'WALC'# Load solar/wind scale factors (copied from tech area sheet on Generators.xlsx)ba_wind_scale = pd.read_excel('wind_solar_ba.xlsx', sheet_name='wind', header=None)ba_solar_scale = pd.read_excel('wind_solar_ba.xlsx', sheet_name='solar', header=None)# Load BPA and CAISO files first (just year 2020 right now)datadir = '.'sel_year = 2020ly = True # 2020 is a leap year...nhrs = 365*24 + 24*lyn5min = 365*24*12 + 24*12*lybpa_wind = rtc.get_reported_ba_scada('BPAT','wind',datadir,hourly=False)bpa_wind = bpa_wind[bpa_wind.index.year==sel_year]caiso_wind = rtc.get_reported_ba_scada('CISO','wind',datadir)caiso_wind = caiso_wind[caiso_wind.index.year==sel_year]caiso_solar = rtc.get_reported_ba_scada('CISO','solar',datadir)caiso_solar = caiso_solar[caiso_solar.index.year==sel_year]# Load EIA fileprint("Loading EIA wind")eia_wind = pd.read_csv('EIA/actual_wind.csv')print("Loading EIA solar")eia_solar = pd.read_csv('EIA/actual_solar.csv')# Convert timestamps to datetimeseia_wind.loc[:,'time_stamp'] = pd.to_datetime(eia_wind['time_stamp'])eia_solar.loc[:,'time_stamp'] = pd.to_datetime(eia_solar['time_stamp'])# Make arrays to fillsolar_hourly = np.zeros((nhrs,len(ba_names)))wind_hourly = np.zeros(solar_hourly.shape)solar_5min = np.zeros((n5min,len(ba_names)))wind_5min = np.zeros(solar_5min.shape)solar_hourly_forecast = np.zeros((nhrs,len(ba_names)))wind_hourly_forecast = np.zeros(solar_hourly.shape)solar_5min_forecast = np.zeros((n5min,len(ba_names)))wind_5min_forecast = np.zeros(solar_5min.shape)utc_to_pst = -8first_time = datetime.datetime(sel_year, 1, 1, 0, 0)first_time = first_time.replace(tzinfo=datetime.timezone.utc)for i, ba in enumerate(ba_names):    ba_cap_wind, ba_cap_solar = 0, 0    wecc_2030_solar, wecc_2030_wind = 0, 0    if ba in ba_wind_scale[0].values:        wecc_2030_wind = ba_wind_scale[ba_wind_scale[0] == ba][1].values[0]        ba_cap_wind = ba_wind_scale[ba_wind_scale[0] == ba][1].values[0]    if ba in ba_solar_scale[0].values:        wecc_2030_solar = ba_solar_scale[ba_solar_scale[0] == ba][1].values[0]        ba_cap_solar = ba_solar_scale[ba_solar_scale[0] == ba][1].values[0]    # BPA uses it's own file    if ba == 'BPAT':        ba_5min_w = bpa_wind.values[:,0]        ba_hourly_w = five_to_hour(bpa_wind.values[:,0])    # All CAISO regions are drawn from CAISO file    elif ba[:2] == 'CI':        ba_5min_w = create_forecast(caiso_wind.values[:,0], 'wind', 5)        ba_hourly_w = create_forecast(five_to_hour(caiso_wind.values[:,0]), 'wind', 60)        ba_5min_s = create_forecast(caiso_solar.values[:,0], 'solar', 5)        ba_hourly_s = create_forecast(five_to_hour(caiso_solar.values[:,0]), 'solar', 60)    # All else come from EIA history    else:        if ba_cap_wind != 0:            ba_hourly_w, ba_5min_w = get_eia(eia_wind, use_ba_wind[ba], nhrs, n5min, first_time, utc_to_pst)            # For BAs that use another BA as reference, use a forecast so profiles aren't identical            if use_ba_wind[ba] != ba:                ba_hourly_w = create_forecast(ba_hourly_w, 'wind', 60)                ba_5min_w = create_forecast(ba_5min_w, 'wind', 5)        else:            ba_hourly_w, ba_5min_w = np.zeros(nhrs), np.zeros(n5min)        if ba_cap_solar != 0:            ba_hourly_s, ba_5min_s = get_eia(eia_solar, use_ba_sol[ba], nhrs, n5min, first_time, utc_to_pst,                                         force_zero=True)            # For BAs that use another BA as reference, use a forecast so profiles aren't identical            if use_ba_sol[ba] != ba:                ba_hourly_s = create_forecast(ba_hourly_s, 'solar', 60)                ba_5min_s = create_forecast(ba_5min_s, 'solar', 5)        else:            ba_hourly_s, ba_5min_s = np.zeros(nhrs), np.zeros(n5min)    solar_hourly[:,i] = ba_hourly_s    wind_hourly[:,i] = ba_hourly_w    solar_5min[:,i] = ba_5min_s    wind_5min[:,i] = ba_5min_w    # Rescale each unit to the capacity in our selected model    print(f"Scaling {ba} solar by:", ba_cap_solar/(np.max(solar_hourly[:,i]) + (np.max(solar_hourly[:,i])==0)), f"{wecc_2030_solar}, {np.max(solar_hourly[:,i])}")    print(f"Scaling {ba} wind by:", ba_cap_wind/(np.max(wind_hourly[:,i]) + (np.max(wind_hourly[:,i])==0)), f"{wecc_2030_wind}, {np.max(wind_hourly[:,i])}")    wind_5min[:,i] *= ba_cap_wind/(np.max(wind_5min[:,i]) + (np.max(wind_5min[:,i])==0))    wind_hourly[:,i] *= ba_cap_wind/(np.max(wind_hourly[:,i]) + (np.max(wind_hourly[:,i])==0))    solar_5min[:,i] *= ba_cap_solar/(np.max(solar_5min[:,i]) + (np.max(solar_5min[:,i])==0))    solar_hourly[:,i] *= ba_cap_solar/(np.max(solar_hourly[:,i]) + (np.max(solar_hourly[:,i])==0))    solar_hourly_forecast[:,i] = create_forecast(solar_hourly[:,i], 'solar', 60)    wind_hourly_forecast[:,i] = create_forecast(wind_hourly[:,i], 'wind', 60)    solar_5min_forecast[:,i] = create_forecast(solar_5min[:,i], 'solar', 5)    wind_5min_forecast[:,i] = create_forecast(wind_5min[:,i], 'wind', 5)    # print("Checking forecasts for", ba)    # print("Solar_hourly")    # plt.plot(solar_hourly[:48,i], color='b')    # plt.plot(solar_hourly_forecast[:48,i], color='g')    # plt.show()    # print("Solar_5min")    # plt.plot(solar_5min[:288,i], color='b')    # plt.plot(solar_5min_forecast[:288,i], color='g')    # plt.show()    # print("Wind_hourly")    # plt.plot(wind_hourly[:48,i], color='b')    # plt.plot(wind_hourly_forecast[:48,i], color='g')    # plt.show()    # print("Wind_5min")    # plt.plot(wind_5min[:288,i], color='b')    # plt.plot(wind_5min_forecast[:288,i], color='g')    # plt.show()# Dump into dataframes and save (This can surely be written much more compactly)solar_df = pd.DataFrame(solar_5min, columns=ba_names)wind_df = pd.DataFrame(wind_5min, columns=ba_names)solar_df_hour = pd.DataFrame(solar_hourly, columns=ba_names)wind_df_hour = pd.DataFrame(wind_hourly, columns=ba_names)solar_df_forecast = pd.DataFrame(solar_5min_forecast, columns=ba_names)wind_df_forecast = pd.DataFrame(wind_5min_forecast, columns=ba_names)solar_df_hour_forecast = pd.DataFrame(solar_hourly_forecast, columns=ba_names)wind_df_hour_forecast = pd.DataFrame(wind_hourly_forecast, columns=ba_names)save_dir = '../market_clearing/system_data'# Save to parquet and csvsolar_df.to_parquet(join(save_dir,'solar_5min_actual.parquet'))wind_df.to_parquet(join(save_dir,'wind_5min_actual.parquet'))solar_df_hour.to_parquet(join(save_dir,'solar_1hr_actual.parquet'))wind_df_hour.to_parquet(join(save_dir,'wind_1hr_actual.parquet'))solar_df.to_csv(join(save_dir,'solar_5min_actual.csv'))wind_df.to_csv(join(save_dir,'wind_5min_actual.csv'))solar_df_hour.to_csv(join(save_dir,'solar_1hr_actual.csv'))wind_df_hour.to_csv(join(save_dir,'wind_1hr_actual.csv'))solar_df_forecast.to_parquet(join(save_dir,'solar_5min_forecast.parquet'))wind_df_forecast.to_parquet(join(save_dir,'wind_5min_forecast.parquet'))solar_df_hour_forecast.to_parquet(join(save_dir,'solar_1hr_forecast.parquet'))wind_df_hour_forecast.to_parquet(join(save_dir,'wind_1hr_forecast.parquet'))solar_df_forecast.to_csv(join(save_dir,'solar_5min_forecast.csv'))wind_df_forecast.to_csv(join(save_dir,'wind_5min_forecast.csv'))solar_df_hour_forecast.to_csv(join(save_dir,'solar_1hr_forecast.csv'))wind_df_hour_forecast.to_csv(join(save_dir,'wind_1hr_forecast.csv'))# Now read in hydro, interpolate to 5 min, and save to parquethydro_1hr = pd.read_excel('market_clearing/system_data/Hydro_2030_8760.xlsx')all_hydro = np.zeros(solar_5min.shape)# Interpolate with cubic splinesfor i, ba in enumerate(ba_names):    hydro_prof = hydro_1hr.loc[:,ba].to_numpy()    x = np.arange(len(hydro_prof))    spline = CubicSpline(x, hydro_prof)    xdense = np.arange(0,len(hydro_prof),1./12)    fit = spline(xdense)    # Trim extreme values    fit[fit<0] = 0    fit[fit>np.max(hydro_prof)] = np.max(hydro_prof)    all_hydro[:,i] = fithydro_5min = pd.DataFrame(all_hydro, columns=ba_names)hydro_5min.to_parquet(join(save_dir,'market_clearing/system_data/hydro_5min.parquet'))hydro_1hr.to_parquet(join(save_dir,'market_clearing/system_data/hydro_1hr.parquet'))hydro_5min.to_csv(join(save_dir,'market_clearing/system_data/hydro_5min.csv'))hydro_1hr.to_csv(join(save_dir,'market_clearing/system_data/hydro_1hr.csv'))